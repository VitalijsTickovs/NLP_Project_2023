{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:32:17.067664Z",
     "end_time": "2023-05-26T00:32:17.113695Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "products = pd.read_csv('products_train.csv')\n",
    "unique_locales = products['locale'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:32:17.072856Z",
     "end_time": "2023-05-26T00:32:24.564927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['DE', 'JP', 'ES', 'FR', 'IT'], dtype=object)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LOCALES TO TRANSLATE\n",
    "unique_locales = np.delete(unique_locales, np.where(unique_locales == 'UK'))\n",
    "unique_locales"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:32:24.564464Z",
     "end_time": "2023-05-26T00:32:24.568556Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# German Machine Translation\n",
    "\n",
    "reference: [English To German Model](https://huggingface.co/google/bert2bert_L-24_wmt_de_en)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert2bert_L-24_wmt_de_en\", pad_token=\"<pad>\", eos_token=\"</s>\", bos_token=\"<s>\", unk_token=\"<unk>\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/bert2bert_L-24_wmt_de_en\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:32:24.570278Z",
     "end_time": "2023-05-26T00:32:51.517309Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (128) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want to drink a как go with me?\n"
     ]
    }
   ],
   "source": [
    "## SILLY EXAMPLE, CAN PLAY WITH THIS IF YOU WANT :)\n",
    "sentence = \"Willst du einen как trinken gehen mit mir?\"\n",
    "\n",
    "input_ids = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "output_ids = model.generate(input_ids)[0]\n",
    "print(tokenizer.decode(output_ids, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:32:51.502732Z",
     "end_time": "2023-05-26T00:33:03.439109Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Real fun part"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    import re    # for regular expressions\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "        text = re.sub(r'[^\\w\\s]', \"\", text)\n",
    "        text = re.sub(r'[^\\w\\d\\s]', '', text)\n",
    "        ## THE TEXT CONTAINED THE SAME GARBAGE AS IN ENGLISH TITLES\n",
    "        ## SHIT DATASET\n",
    "        # I HATE LIFE\n",
    "        text = re.findall(r'\\b(?:[a-zA-Z]+|\\d+)\\b', text)\n",
    "        text = \" \".join(text)\n",
    "        # removing non-unicode characters\n",
    "        # for example, it will remove character 'Ç'\n",
    "        non_ascii_pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "        text = non_ascii_pattern.sub('', text)\n",
    "\n",
    "        # Tokenize it\n",
    "        return text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "    # Return a string"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:33:03.443212Z",
     "end_time": "2023-05-26T00:33:03.443834Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0          RED DRAGON Amberjack 3 - Steel Tip 22 Gramm Wo...\n1          Simply Keto Lower Carb* Schokodrops ohne Zucke...\n2          Sennheiser 508377 PC 5.2 Chat, Stilvolles Mult...\n3          AmyBenton Auto ab 1 2 3 ahre - Baby Aufziehbar...\n4              PLAYMOBIL - 70522 - Cavaliere mit grauem Pony\n                                 ...                        \n1551052    Barbie - Playset Gelateria con Bambola con Mac...\n1551053    Braun Silk-épil 1 Depilatore Donna, Epilatore ...\n1551054    BoxLegend Sacchetti Sottovuoto Vestiti 6 Pezzi...\n1551055    Trasportino Pratiko Metal - Accessorio da viag...\n1551056    LiCB - Batterie LR1130, batterie alcaline AG10...\nName: title, Length: 1551049, dtype: object"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get all the german rows\n",
    "locale = 'DE'\n",
    "de_products = products.loc[products['locale'] == locale]\n",
    "# Get only titles\n",
    "de_products_title = products['title']\n",
    "# drop all nan rows\n",
    "de_products_title.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:33:03.515365Z",
     "end_time": "2023-05-26T00:33:05.257826Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# a bit of preprocessing\n",
    "# will update it\n",
    "de_products_title = de_products_title.apply(process)\n",
    "# converting to list to translate the rows\n",
    "de_products_title_list = de_products_title.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:33:05.253682Z",
     "end_time": "2023-05-26T00:33:23.011136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "### DUMB DEBUGGER\n",
    "\n",
    "## REAL FUN\n",
    "## NOT FUN ACTUALLY\n",
    "# decoded_sequences = []\n",
    "# for i in range(161+102, len(de_products_title_list)):\n",
    "#     input_ids = tokenizer(de_products_title_list[i], padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "#     output_ids = model.generate(input_ids)[0]\n",
    "#     decoded_sequences.append(tokenizer.decode(output_ids, skip_special_tokens=True))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:33:23.014835Z",
     "end_time": "2023-05-26T00:33:23.016123Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "## REAL REAL FUN\n",
    "## CHECK OUTPUT :)\n",
    "input_ids = tokenizer(de_products_title_list, padding=True, return_tensors=\"pt\", add_special_tokens=False).input_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:42:11.775317Z",
     "end_time": "2023-05-26T00:51:58.138588Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:33:23.054875Z",
     "end_time": "2023-05-26T00:34:23.791778Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m decoded_sequences \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241m.\u001B[39mbatch_decode(output_ids, skip_special_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "decoded_sequences = tokenizer.batch_decode(output_ids, skip_special_tokens = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:41:28.116644Z",
     "end_time": "2023-05-26T00:41:28.119941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m translated \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mDataFrame(decoded_sequences, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      2\u001B[0m translated\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgerman-to-english.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "translated = pd.DataFrame(decoded_sequences, columns=['title'])\n",
    "translated.to_csv('german-to-english.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoded_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdecoded_sequences\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'decoded_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "print(decoded_sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-26T00:41:29.846333Z",
     "end_time": "2023-05-26T00:41:29.853100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
